---
title: "Mixture of Balls With Different Volumes"
author: 
   - NIANG Mohamed
   - KAINA Mohamed Abdellah 
date: "20 December 2019"
output:
  pdf_document: 
    fig_caption: yes
    highlight: haddock
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document: 
    df_print: kable
    highlight: haddock
    number_sections: yes
    theme: cerulean
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Context

Let us consider a vector of p random variables $x^1,...,x^p$ independent, normal, all with mean 0 and variance $\sigma^2$. The random vector $x = (x^1,...,x^p)'$ is normal with mean vector $(0,...,0)^\intercal$, and covariance matrix $\sigma^2I_p$. This distribution defines a gaussian ball with mean vector $(0,...,0)^\intercal$, and covariance matrix $\sigma^2I_p$.
Let us consider a mixture of K gaussian balls:
$$
\begin{aligned}
 p(x|\pi,\mu_1,...,\mu_K,\sigma_1,...,\sigma_K) = \sum_{k=1}^K \pi_k\mathcal{N}_p(x|\mu_k,\,\Sigma_k = \sigma^2_kI_p)\
\end{aligned}
$$
Where $\pi = {\pi_k}$ are the proportions of the mixture.
In the following, we will consider: 

* a sample $X = (x_1,...,x_n)$ from the above defined mixture ;
* latent variables $Z = (z_1,...,z_n)$ indicating from which component of the mixture each x_i originates ;
* the vector of parameters is denoted $\theta = (\pi,\mu_1,...,\mu_K,\sigma_1,...,\sigma_K)$. For simplicity, $\theta_j = \{\pi_j, \mu_j, \Sigma_j\}$.

# Problem 

## Load Library

```{r}
library(knitr) # Markdown 
library(kableExtra)
# For Exercise 1
library(mvtnorm)
library(ggplot2)
# For Exercise 2
library(mclust)
library(tidyverse)
library(factoextra)
library(fpc)
# For Exercise 5
library(NbClust)
library(cluster)
library(e1071)
```

## Exercise 1: Simulation

### Data Simulation

```{r}
set.seed(1234)
X1 <- rmvnorm(1000,mean = c(1,2),sigma = diag(c(1,1)))
X2 <- rmvnorm(1000,mean = c(1,2),sigma = 4*diag(c(1,1)))
Y <- rep(c(1,2),c(1000,1000))
p1 <- 0.5
p2 <- 1 - p1
mixed_proportion <- c(p1,p2)
```

### Display the Sample

```{r}
mydata <- data.frame(cbind(rbind(X1,X2),Y))
names(mydata) <- c("X1","X2","Y")
mydata$Y <- as.factor(mydata$Y)
attach(mydata)
```

```{r}
# Display the Sample
head(mydata)
```

### Display the contour plot of the two dimensional density

```{r}
ggplot(mydata, aes(X1, X2, color = Y)) +
  geom_point(aes(shape=Y), size = 2) +
  geom_density_2d(aes(colour = Y), size = 1) + 
  scale_color_manual(values = c("blue", "red"))
```

## Exercise 2: Mclust Versus Kmeans

### Running Mclust on the Simulated Data From The Exercise 1

```{r}
mclust <- Mclust(mydata[,c("X1","X2")])
```

```{r}
summary(mclust) # Model = VII and Number of Components = 2
```

```{r}
# BIC values used for choosing the number of clusters
fviz_mclust(mclust, "BIC", palette = "jco")
```

```{r}
# Classification: plot showing the clustering
fviz_mclust(mclust, "classification", geom = "point",
pointsize = 1.5, palette = "jco")
```

**Comments:**
The model given by 'Mclust' object is $(VII,2)$. Indeed, in the first exercise, the variance in the two distributions is proportional to the identity matrix. Thus the analysis of the results shows that the best model is the VII because it has the largest BIC. And we observe a sharp drop in the BIC from the second component. As a result, we take $K = 2$ as the number of classes.

### Parameter estimation 

```{r}
# The estimated parameters 
mclust_VII <- Mclust(mydata[,c("X1","X2")],modelNames = "VII",G=2)
```

```{r}
param <- mclust_VII$parameters
param

param_mean <- mclust_VII$parameters$mean
param_mean

param_sigma <- mclust_VII$parameters$variance$sigma
param_sigma
```

### A Partition of The Simulated Data Using Mclust

```{r}
mclust_part <- MclustDR(mclust_VII)
```

```{r}
summary(mclust_part)
```

```{r}
clusterPlots <- data.frame(X1 = mydata[,c("X1")], 
                           X2 = mydata[,c("X2")], 
                           cluster = factor(mclust_part$classification, 
    levels = c(1, 2), labels = c("Y = 1", "Y = 2")))

clusterPlots.gg <- ggplot(clusterPlots)
clusterPlots.gg + geom_point(aes(x = X1, y = X2, color = cluster))
```

### A Partition of The Simulated Data Using Kmeans

```{r}
kmeans_part <- kmeans(mydata[,c("X1","X2")], centers = 2) 
```

```{r}
summary(kmeans_part)
```

```{r}
fviz_cluster(kmeans_part, data = mydata[,c("X1","X2")])
```

### Comparison of The Results of The Two Partitions (Kmeans and Mclust)

To compare the two models, we use the $cluster.stats()$ function of the fpc library. Among the values returned by the function $cluster.stats()$, there are two indices to compare the performance of two clusters, namely the within.cluster.ss and the means.silwidth.

Most often, we focus on using within.cluster.ss and avg.silwidth to validate the clustering method. The within.cluster.ss measurement stands for the within clusters sum of squares, and avg.silwidth represents the average silhouette width.

* within.cluster.ss measurement shows how closely related objects are in clusters; the smaller the value, the more closely related objects are within the cluster.
* avg.silwidth is a measurement that considers how closely related objects are within the cluster and how clusters are separated from each other. The silhouette value usually ranges from 0 to 1; a value closer to 1 suggests the data is better clustered.

```{r}
d <- dist(mydata[,c("X1","X2")], method ="euclidean")
stat_mclust <- cluster.stats(d, mclust_VII$classification)
stat_kmeans <- cluster.stats(d, kmeans_part$cluster)

within_mclust <- stat_mclust$within.cluster.ss
avg_mclust <- stat_mclust$avg.silwidth

within_kmeans <- stat_kmeans$within.cluster.ss
avg_kmeans <- stat_kmeans$avg.silwidth

statsmodelsMclust <- c(within_mclust, avg_mclust)
statsmodelsKmeans <- c(within_kmeans, avg_kmeans)
resultsMclust <- data.frame("Mclust" = c("within.cluster.ss","avg.silwidth"), "Stats Mclust" = statsmodelsMclust)
resultsKmeans <- data.frame("Kmeans" = c("within.cluster.ss","avg.silwidth"), "Stats Kmeans" = statsmodelsKmeans)
resultfinal <- cbind(resultsMclust,resultsKmeans)
```

```{r}
# Comparison Table
kable(arrange(resultfinal,desc(statsmodelsMclust),desc(statsmodelsKmeans)), digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                font_size = 18,
                position = "center")
```

**Comments:**
Based on the results of the above table, we conclude that the Kmeans is **the best model because it has within.cluster.ss smaller and one avg.silwidth larger than the Mclust**.

## Exercise 3: EM Algorithm for a Mixture of Balls

### Detail the Computation

Given our current estimate of the parameters $\theta^{(q)}$, the conditional distribution of the $z_i$ is determined by Bayes theorem to be normalized Gaussian density weighted by $\pi_j$:
$$
\begin{aligned}
& t_{ik}^{(q)} \\
= \ \ & p(z_i = k \mid x_i; \boldsymbol{\theta}^{(q)}) \\
= \ \ & \frac{p(x_i, z_i = k ; \boldsymbol{\theta}^{(q)})}{p(x_i; \boldsymbol{\theta}^{(q)})} \\
= \ \ & \frac{p(x_i, z_i = k ; \boldsymbol{\theta}^{(q)})}{\sum_{j = 1}^{K} p(x_i, z_i = j; \boldsymbol{\theta}^{(q)})} \\
= \ \ & \frac{f\left(x_i ; \mu_k^{(q)}, \Sigma_k^{(q)}\right) \cdot \pi_{k}^{(q)}}{\sum_{j = 1}^{K} f\left(x_i ; \mu_j^{(q)}, \Sigma_j^{(q)}\right) \cdot \pi_{j}^{(q)}}
\end{aligned}
$$

### Express The Expectation Step

**Expectation-Step**

Define $Q(\boldsymbol{\theta}^{(q)}\mid\boldsymbol{\theta})$

$$
L(\mathbf{X}, \mathbf{Z};\theta) = \sum_{i=1}^{n} \log f\left(x_i ; \mu_k, \Sigma_k\right) \cdot \pi_{k}
$$

Where

$$
f\left(x_i ; \mu_k, \Sigma_k\right) = \frac{1}{(2 \pi)^{d / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_i-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_i-\mu_{k}\right)\right)
$$

Then

$$
Q(\boldsymbol{\boldsymbol{\theta}^{(q)} \mid \theta}) = \sum_{i=1}^{n} \sum_{k=1}^{K} p(z_i = k \mid x_i; \boldsymbol{\theta}^{(q)}) \cdot \log f\left(x_i ; \mu_k, \Sigma_k\right) \cdot \pi_k
$$

### Detail The Computation of Maximization Step

**Maximization-Step**

We need to maximize, with respect to our parameters $\boldsymbol{\theta}^{(q)}$, the quantity

$$
\begin{aligned}
& Q(\boldsymbol{\theta}^{(q)} \mid \boldsymbol{\theta}) \\
= \ \ & \sum_{i=1}^{n}\sum_{k=1}^{K} t_{ik}^{(q)} \log \frac{1}{(2 \pi)^{d / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_i-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x_i-\mu_{k}\right)\right) \cdot \pi_{k} \\
= \ \ & \sum_{i=1}^{n}\sum_{k=1}^{K} t_{ik}^{(q)} \left[\log \pi_k - \frac{1}{2} \log \left|\Sigma_{k}\right| - \frac{1}{2}(x_i-\mu_k)^T \Sigma_{k}^{-1}(x_i - \mu_k) - \frac{d}{2} \log(2\pi) \right]
\end{aligned}
$$

* Update $\pi_k$

Grouping together only the terms that depend on $\pi_k$, we find that we need to maximize

$$
\sum_{i=1}^{n}\sum_{k=1}^{K} t_{ik}^{(q)} \log \pi_k
$$

With subject to

$$
\sum_{k=1}^{K} \pi_k = 1
$$

So we construct the Lagrangian

$$
\mathcal{L}(\pi) = \sum_{i=1}^{n}\sum_{k=1}^{K} t_{ik}^{(q)} \log \pi_k + \lambda \left( \sum_{k=1}^{K} \pi_k - 1 \right)
$$

Where $\lambda$ is the Lagrange multiplier. Taking derivative, we find

$$
\frac{\partial}{\partial \pi_{k}} \mathcal{L}(\pi)=\sum_{i=1}^{n} \frac{t_{ik}^{(q)}}{\pi_{k}}+\lambda
$$

Setting this to zero and solving, we get

$$
\pi_{k}=\frac{\sum_{i=1}^{n} t_{ik}^{(q)}}{-\lambda}
$$

Using the constraint that $\sum_{k=1}^{K} \pi_k = 1$ and knowing the fact that $\sum_{k=1}^{K} t_{ik}^{(q)} = 1$ (probabilities sum to 1), we easily find:

$$
-\lambda=\sum_{i=1}^{n} \sum_{k=1}^{K} t_{ik}^{(q)} = \sum_{i=1}^{n} 1= n
$$

We therefore have updates for the parameters $\pi_k$:

$$
\pi_{k} =\frac{1}{n} \sum_{i=1}^{n} t_{ik}^{(q)}
$$

* Update $\mu_k$

$$
\begin{aligned}
& \frac{\partial}{\partial \mu_{k}} Q(\boldsymbol{\theta}^{(q)} \mid \boldsymbol{\theta}) \\
= \ \ & \frac{\partial}{\partial \mu_{k}} \sum_{i=1}^{n} -\frac{1}{2} t_{ik}^{(q)} (x_i-\mu_k)^T \Sigma_{k}^{-1}(x_i - \mu_k) \\
= \ \ & \frac{1}{2} \sum_{i=1}^{n} t_{ik}^{(q)} \dfrac{\partial}{\partial \mu_{k}} \left( 2 \mu_k^T\Sigma_k^{-1}x_i - \mu_k^T\Sigma_k^{-1}\mu_k \right) \\
= \ \ & \sum_{i=1}^{n} t_{ik}^{(q)} \left( \Sigma_k^{-1} x_i - \Sigma_k^{-1}\mu_k \right)
\end{aligned}
$$

Setting this to zero and solving for $\mu_k$ therefore yields the update rule

$$
\mu_{k} =\frac{\sum_{i=1}^{n} t_{ik}^{(q)} x_i}{\sum_{i=1}^{n} t_{ik}^{(q)}}
$$


* Update $\Sigma_k$

$$
\begin{aligned}
& \frac{\partial}{\partial \Sigma_{k}} Q(\boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}) \\
= \ \ & \frac{\partial}{\partial \Sigma_{k}} \sum_{i=1}^{n} -\frac{1}{2} t_{ik}^{(q)} \left[ \log\left|\Sigma_{k}\right| + (x_i-\mu_k)^T \Sigma_{k}^{-1}(x_i - \mu_k)\right] \\
= \ \ & -\frac{1}{2} \sum_{i=1}^{n} t_{ik}^{(q)} \dfrac{\partial}{\partial \Sigma_{k}} \left[ \log\left|\Sigma_{k}\right| + (x_i-\mu_k)^T \Sigma_{k}^{-1}(x_i - \mu_k)\right] \\
= \ \ & -\frac{1}{2} \sum_{i=1}^{n} t_{ik}^{(q)} \left( \Sigma_{k}^{-1} - (x_i-\mu_k)(x_i - \mu_k)^T \Sigma_{k}^{-2} \right)
\end{aligned}
$$

Setting the partial derivative to zero and solving for $\Sigma_k$ therefore yields the update rule

$$
\Sigma_{k} =\frac{\sum_{i=1}^{n} t_{ik}^{(q)} (x_i-\mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{n} t_{ik}^{(q)}}
$$

### Write The Pseudo-Code of An EM Algorithm

```{r}
# Initialization Function
init.EM <- function(X,K=n){
  mus <- X[sample(1:nrow(X),K),1]
  sds <- rep(sd(X[,1]),K)
  pis <- rep(1/K,K)
  return(parameters = list(mus=mus,sds=sds,pis=pis))
}
```

### Write a Expectation Step Function

```{r}
# Expectation Function
E.step <- function(X,parameters){
  K <- length(parameters$mus)
  Tik <- matrix(0,nrow(X),K)
  for(k in 1:K){
    Tik[,k] <- parameters$pis[k]*dnorm(X[,1],
                                     mean=parameters$mus[k],
                                     sd=parameters$sds[k])
  }
  return(Tik <- Tik/rowSums(Tik))
}
```

### Write a Maximization Step Function

```{r}
# Maximization Function
M.step <- function(X, Tik, parameters){
  K <- length(parameters$mus)
  parameters$pis <- colSums(Tik)/nrow(X)
  for (k in 1:K){
    parameters$mus[k] <- sum(Tik[,k]*X) / sum(Tik[,k])
    parameters$sds[k] <- sqrt(sum(Tik[,k]*(X-parameters$mus[k])^2)/ sum(Tik[,k]))
  }
  return(parameters)
}
```

### Program The EM Algorithm

```{r}
# EM Algorithm
EM <- function(X,K=n){
  parameters <- init.EM(X,K)
  iter <- 0
  parameters.new <- parameters
  repeat{
    Tik <- E.step(X,parameters)
    parameters.new <- M.step(X,Tik,parameters)
    if ((sum(unlist(parameters.new) - unlist(parameters))^2)/
        sum(unlist(parameters.new))^2 < 1e-20) break
    parameters<-parameters.new
  }
  return(list(parameters=parameters.new,Tik=Tik))
}
```

### Application On The Simulated Data From The Exercise 1

```{r}
# Define Mixture Data From Exercise 1
mixture_data <- mydata[,c("X1","X2")]
```

```{r}
# Application
pc <- proc.time()
result1 <- EM(mixture_data,2)$parameters
proc.time() - pc
result1
```

## Exercise 4: Mixture of Balls Using The Kernel Trick

Assume a transformation $\Phi : \mathbb{R} \leftarrow \mathcal{V}$ and denote $K(.,.)$ the scalar product in $\mathcal{V}$.

### Compute The Distance in The Transformed Space

Let $\mu_k = {\frac{1}{\sum_j t_{jk}}}{\sum_j t_{jk}x_j}$. Let's show that the distance between $x_i$ and $\mu_k$ in the transformed space is :

$$
\|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V} = K(x_i,x_i) - {\frac{2}{\sum_j t_{jk}}} \sum_j t_{jk} K(x_i,x_j) + {\frac{1}{\sum_j t_{jk}   \sum_h t_{hk}}} \sum_j \sum_h  t_{jk} t_{hk} K(x_j, x_h)
$$

Based on the definition of the scalar product, we have

$$
\|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V} = \langle {x_i - \mu_k,x_i - \mu_k} \rangle_\mathcal{V}
$$

$$
\|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V} = \langle {x_i,x_i} \rangle_\mathcal{V} + \langle {x_i,-\mu_k} \rangle_\mathcal{V} + \langle {-\mu_k,x_i} \rangle_\mathcal{V} + \langle {-\mu_k,-\mu_k} \rangle_\mathcal{V}
$$

By developing the scalar product, we obtain

$$
\|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V} = \langle {x_i,x_i} \rangle_\mathcal{V} - 2\langle {x_i,\mu_k} \rangle_\mathcal{V} + \langle {\mu_k,\mu_k} \rangle_\mathcal{V} 
$$

Yet

$$
\langle {x_i,x_i} \rangle_\mathcal{V} = K(x_i,x_i) \qquad (1)
$$

$$
\langle {x_i,\mu_k} \rangle_\mathcal{V} = \langle {x_i,{\frac{1}{\sum_j t_{jk}}}{\sum_j t_{jk}x_j}} \rangle_\mathcal{V}
$$

$$
\langle {x_i,\mu_k} \rangle_\mathcal{V} = {\frac{1}{\sum_j t_{jk}}}{\sum_j t_{jk}}\langle {x_i,x_j} \rangle_\mathcal{V}
$$

$$
\langle {x_i,\mu_k} \rangle_\mathcal{V} = {\frac{1}{\sum_j t_{jk}}}{\sum_j t_{jk}}K(x_i,x_j) \qquad (2)
$$

And

$$
\langle {\mu_k,\mu_k} \rangle_\mathcal{V} = \langle {{\frac{1}{\sum_j t_{jk}}}{\sum_j t_{jk}x_j},{\frac{1}{\sum_h t_{hk}}}{\sum_h t_{hk}x_h}} \rangle_\mathcal{V}
$$

$$
\langle {\mu_k,\mu_k} \rangle_\mathcal{V} = {\frac{1}{\sum_j t_{jk}}}{\sum_j t_{jk} {\frac{1}{\sum_h t_{hk}}}{\sum_h t_{hk} \langle {x_j},x_h}} \rangle_\mathcal{V}
$$

$$
\langle {\mu_k,\mu_k} \rangle_\mathcal{V} ={\frac{1}{\sum_j t_{jk}   \sum_h t_{hk}}} \sum_j \sum_h  t_{jk} t_{hk} K(x_j, x_h) \qquad (3)
$$

According to (1), (2) and (3), we can see that

$$
\|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V} = K(x_i,x_i) - {\frac{2}{\sum_j t_{jk}}} \sum_j t_{jk} K(x_i,x_j) + {\frac{1}{\sum_j t_{jk}   \sum_h t_{hk}}} \sum_j \sum_h  t_{jk} t_{hk} K(x_j, x_h)
$$

### Express The Estimation of Variance

The estimate of $\sigma_k$ in the transformed space is given by 

$$
\sigma_k = {\frac{1}{\sum_i t_{ik}}} \sum_i \sum_k  t_{ik} \|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V}
$$

### Compute The Responsabilities In The Transformed Space

Let the objective function $J = \sum_i \sum_k  t_{ik} \|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V}$. The computation of responsabilities is given by 

$$
\frac{\partial J}{\partial t_{ik}}  = \sum_i \sum_k \|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V}
$$

This gives you 

$$
t_{ik}  = \left\{
    \begin{array}{ll}
        1 & \mbox{if } k =  \operatorname*{argmin}\|{\boldsymbol x_i} -\boldsymbol \mu_k\|^2_\mathcal{V} \\
        0 & \mbox{otherwise}
    \end{array}
\right.
$$

## Exercise 5: The Iris Data

We will be using the **Iris Dataset** for the comparison of clustering algorithms. The dataset consists of 150 observations with 4 variables: Sepal Length, Sepal Width, Petal Length and Petal Width. The entire dataset has three different species of Setosa, Versicolor and Virginica with 50 samples each.

### Run Unsupervised Algorithms on The iris dataset

Clustering is an unsupervised machine learning technique to identify groups in the dataset which contain observations with similar profiles according to the specified criteria. Similarity between the observations are defined using distance measures such as Euclidian, Manhattan Distances and some correlation based distance measures.

There are different clustering methodologies, which can be subdivided as the five general strategies with examples:

* Partitioning Methods- K Means Clustering

* Hierarchical Methods- Hierarchical Clustering

* Fuzzy Clustering- Fuzzy C-Means Clustering

* Model- Based Clustering- Normal Mixture Model Clustering

Before we move on to Clustering methodologies, we need to find the optimal number of clusters as certain methods require prior knowledge of number of clusters before performing the clustering methodology.

#### Kmeans

```{r}
set.seed(1234)
irisdata <- iris[,-5]
iris.scaled <- scale(irisdata)
```

```{r}
head(iris.scaled)
```

```{r}
# The Number of Clusters Required
nb <- NbClust(iris.scaled, distance = "euclidean", min.nc = 2,
              max.nc = 10, method = "complete", index ="all")
```

```{r}
fviz_nbclust(nb) + theme_minimal()
```

```{r}
irisKmeans <- kmeans(iris.scaled, 3, nstart = 20)
```

```{r}
fviz_cluster(irisKmeans, data = irisdata, geom = "point",
             stand = FALSE, ellipse.type = "norm")
```

```{r}
# Stats Kmeans
iris.dist <- dist(iris.scaled)
stat_kmeans <- cluster.stats(iris.dist, irisKmeans$cluster)
within_kmeans <- stat_kmeans$within.cluster.ss
avg_kmeans <- stat_kmeans$avg.silwidth
```

#### Heirarchical Clustering With Ward's Method

Hierarchical clustering can be divided into agglomerative and divisive clustering. The former starts with a single element and builds the nodes based on similar clusters to form a final big cluster or root. The latter starts with a big cluster and breaks it until all data points are in different clusters.

**Ward's Method**

This minimizes the total within-cluster variance. At each of the steps, the pairs of clusters with minimum between-cluster distances are merged.

```{r}
irisHclust <- hclust(iris.dist, method="ward.D")
```

```{r}
fviz_dend(irisHclust, k = 3, # Cut in three groups
cex = 0.5, # label size
k_colors = c("#2E9FDF", "#00AFBB", "#E7B800"),
color_labels_by_k = TRUE, # color labels by groups
rect = TRUE # Add rectangle around groups
)
```

```{r}
# Cut tree into 3 groups
grp <- cutree(irisHclust, k = 3)
```

```{r}
fviz_cluster(list(data = iris.scaled, cluster = grp),
palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
ellipse.type = "convex", # Concentration ellipse
repel = TRUE, # Avoid label overplotting (slow)
show.clust.cent = FALSE, ggtheme = theme_minimal())
```

#### Fuzzy C-Means Clustering

In K-means, the data is divided into distinct clusters, where each element is affected exactly to one cluster. This type of clustering is also known as hard clustering or non-fuzzy clustering.

Unlike K-means, Fuzzy clustering is considered as a soft clustering, in which each element has a probability of belonging to each cluster. In other words, each element has a set of membership coefficients corresponding to the degree of being in a given cluster.

Points close to the center of a cluster, may be in the cluster to a higher degree than points in the edge of a cluster. The degree, to which an element belongs to a given cluster, is a numerical value in [0, 1].

Fuzzy c-means (FCM) algorithm is one of the most widely used fuzzy clustering algorithms. It was developed by Dunn in 1973 and improved by Bezdek in 1981. It's frequently used in pattern recognition.

```{r}
irisCmeans <- cmeans(iris.scaled, 3)
```

```{r}
fviz_cluster(list(data = iris.scaled, cluster=irisCmeans$cluster))
```

```{r}
# Stats Cmeans
stat_cmeans <- cluster.stats(iris.dist, irisCmeans$cluster)
within_cmeans <- stat_cmeans$within.cluster.ss
avg_cmeans <- stat_cmeans$avg.silwidth
```

#### Normal Mixture Model Clustering

The traditional clustering methods such as hierarchical clustering and partitioning algorithms (k-means and others) are heuristic and are not based on formal models.

An alternative is to use model-based clustering, in which, the data are considered as coming from a distribution that is mixture of two or more components (i.e. clusters) (Chris Fraley and Adrian E. Raftery, 2002 and 2012).

Each component k (i.e. group or cluster) is modeled by the normal or Gaussian distribution which is characterized by the parameters: mean vector and covariance matrix, an associated probability in the mixture. Each point has a probability of belonging to each cluster.

```{r}
irisMclust <- Mclust(iris.scaled,G=3)
```

```{r}
summary(irisMclust)
```

```{r}
fviz_cluster(list(data = iris.scaled, cluster=irisMclust$classification))
```

```{r}
# Stats Mclust
stat_mclust <- cluster.stats(iris.dist, irisMclust$classification)
within_mclust <- stat_mclust$within.cluster.ss
avg_mclust <- stat_mclust$avg.silwidth
```

### Comment : Kmeans Versus Cmeans

```{r}
statsmodelsCmeans <- c(within_cmeans, avg_cmeans)
statsmodelsKmeans <- c(within_kmeans, avg_kmeans)
resultsCmeans <- data.frame("Cmeans" = c("within.cluster.ss","avg.silwidth"), "Stats Cmeans" = statsmodelsCmeans)
resultsKmeans <- data.frame("Kmeans" = c("within.cluster.ss","avg.silwidth"), "Stats Kmeans" = statsmodelsKmeans)
resultfinal <- cbind(resultsCmeans,resultsKmeans)
```

```{r}
# Comparison Table
kable(arrange(resultfinal,desc(statsmodelsCmeans),desc(statsmodelsKmeans)), digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                font_size = 18,
                position = "center")
```

**Comments:**
Based on the results of the above table, we conclude that the Kmeans is **the best model because it has within.cluster.ss smaller and one avg.silwidth larger than the Cmeans**.